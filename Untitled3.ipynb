{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haticeeakgull/Smart-Parking-Lot-System-/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RysJkUHz2lVS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "# --- Google Drive Bağlantısı (Colab için Gerekli) ---\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from meteostat import Point, Daily, Hourly\n",
        "from datetime import datetime ,timedelta\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install meteostat\n"
      ],
      "metadata": {
        "id": "BQbDCvaZ4X73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69747594-d317-4df5-e600-4ce726dc297b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: meteostat in /usr/local/lib/python3.12/dist-packages (1.7.6)\n",
            "Requirement already satisfied: pandas>=2 in /usr/local/lib/python3.12/dist-packages (from meteostat) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from meteostat) (2025.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from meteostat) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2->meteostat) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2->meteostat) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2->meteostat) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Düzeltilmiş Dosya Yolu\n",
        "# Klasörünüzün ismini buraya tam olarak yazmalısınız (örneğin: 'LizbonParkVeri')\n",
        "# Eğer ana dizinde (MyDrive) değilse, klasör adını doğru verin.\n",
        "# Varsayalım ki klasör adınız 'ParkingData'\n",
        "base_folder_name = 'parkingDataset' # <-- LÜTFEN KENDİ KLASÖR İSMİNİZİ YAZIN\n",
        "base_path = os.path.join('/content/drive/MyDrive', base_folder_name)\n",
        "\n",
        "file_names = [\n",
        "    '1t2020.csv', # Varsayım: Ocak, Şubat, Mart verileri\n",
        "    '2t2020.csv', # Varsayım: Nisan, Mayıs, Haziran verileri\n",
        "    '4t2020.csv'  # Varsayım: Ekim, Kasım, Aralık verileri\n",
        "]\n",
        "\n",
        "all_data_frames = []\n",
        "\n",
        "# --- Dosyaları Döngüyle Okuma ve Birleştirme ---\n",
        "print(\"Veri setleri okunuyor ve birleştiriliyor...\")\n",
        "for name in file_names:\n",
        "    file_path = os.path.join(base_path, name)\n",
        "    try:\n",
        "        # CSV'yi okurken formatı belirtmek faydalı olabilir\n",
        "        df_temp = pd.read_csv(file_path,sep=\";\")\n",
        "        all_data_frames.append(df_temp)\n",
        "        print(f\"'{name}' başarıyla yüklendi. Boyut: {df_temp.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"UYARI: '{name}' dosyası bulunamadı. Lütfen dosya yolunu kontrol edin.\")\n",
        "    except Exception as e:\n",
        "        print(f\"'{name}' okunurken hata oluştu: {e}\")\n",
        "\n",
        "# Tüm veri setlerini tek bir DataFrame'de birleştirme\n",
        "if all_data_frames:\n",
        "    df_full = pd.concat(all_data_frames, ignore_index=True)\n",
        "    print(\"\\n--- BİRLEŞTİRİLMİŞ VERİ SETİNİN İLK 5 SATIRI ---\")\n",
        "    print(df_full.head())\n",
        "    print(f\"\\nToplam Satır Sayısı: {len(df_full)}\")\n",
        "else:\n",
        "    print(\"Hata: Hiçbir dosya başarıyla yüklenemedi. İşleme devam edilemez.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQsaofXfh_ZB",
        "outputId": "6ddc5c15-2776-4152-ec47-b5d1cee655c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Veri setleri okunuyor ve birleştiriliyor...\n",
            "'1t2020.csv' başarıyla yüklendi. Boyut: (100000, 7)\n",
            "'2t2020.csv' başarıyla yüklendi. Boyut: (100039, 7)\n",
            "'4t2020.csv' başarıyla yüklendi. Boyut: (100000, 7)\n",
            "\n",
            "--- BİRLEŞTİRİLMİŞ VERİ SETİNİN İLK 5 SATIRI ---\n",
            "  id_park                 name  max_capacity  occupancy  \\\n",
            "0    P028     Chão do Loureiro           192          0   \n",
            "1    P044   Telheiras Nascente           109         25   \n",
            "2    P013           Monumental           277        195   \n",
            "3    P004      Atrium Saldanha           251          0   \n",
            "4    P040  Mercado de Alvalade           118         88   \n",
            "\n",
            "                                            position          datetime  \\\n",
            "0  {'coordinates': [-9.135017, 38.712416], 'type'...  31/12/2019 23:59   \n",
            "1  {'coordinates': [-9.164886, 38.761512], 'type'...  31/12/2019 23:59   \n",
            "2  {'coordinates': [-9.14430800000002, 38.734036]...  31/12/2019 23:59   \n",
            "3  {'coordinates': [-9.1446021616448, 38.73326873...  31/12/2019 23:59   \n",
            "4  {'coordinates': [-9.139498, 38.755424], 'type'...  01/01/2020 00:03   \n",
            "\n",
            "          entity_ts  \n",
            "0  2020/01/01 00:00  \n",
            "1  2020/01/01 00:00  \n",
            "2  2020/01/01 00:00  \n",
            "3  2020/01/01 00:01  \n",
            "4  2020/01/01 00:05  \n",
            "\n",
            "Toplam Satır Sayısı: 300039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Mevcut Sütun İsimleri ---\")\n",
        "print(df_full.columns.tolist())\n",
        "df_full['datetime'] = pd.to_datetime(df_full['datetime'], format='%Y/%m/%d %H:%M', errors='coerce')\n",
        "df_full = df_full.drop(columns=['position', 'entity_ts'])\n",
        "df_full['hour'] = df_full['datetime'].dt.hour\n",
        "df_full['dayofweek'] = df_full['datetime'].dt.dayofweek # Pazartesi=0, Pazar=6\n",
        "df_full['is_weekend'] = (df_full['dayofweek'] >= 5).astype(int) # Hafta Sonu (Cmt/Pazar) = 1\n",
        "\n",
        "print(\"\\n--- 3. Adım Sonrası Veri Seti Bilgileri ---\")\n",
        "df_full.info()\n",
        "print(\"\\n--- Yeni Özellikler ve Son 5 Satır ---\")\n",
        "print(df_full[['datetime', 'hour', 'dayofweek', 'is_weekend', 'occupancy']].tail())\n",
        "\n",
        "# Eksik (NaT) değer olup olmadığını kontrol edelim\n",
        "missing_times = df_full['datetime'].isnull().sum()\n",
        "if missing_times > 0:\n",
        "    print(f\"\\n*** DİKKAT: {missing_times} adet geçersiz tarih/saat bulundu (NaT). Bu satırlar temizlenmelidir. ***\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H1BnrEZoNxN",
        "outputId": "d3191f2d-1edb-4d72-cdaa-dfa5a417a249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Mevcut Sütun İsimleri ---\n",
            "['id_park', 'name', 'max_capacity', 'occupancy', 'position', 'datetime', 'entity_ts']\n",
            "\n",
            "--- 3. Adım Sonrası Veri Seti Bilgileri ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 300039 entries, 0 to 300038\n",
            "Data columns (total 8 columns):\n",
            " #   Column        Non-Null Count   Dtype         \n",
            "---  ------        --------------   -----         \n",
            " 0   id_park       300039 non-null  object        \n",
            " 1   name          300039 non-null  object        \n",
            " 2   max_capacity  300039 non-null  int64         \n",
            " 3   occupancy     300039 non-null  int64         \n",
            " 4   datetime      204387 non-null  datetime64[ns]\n",
            " 5   hour          204387 non-null  float64       \n",
            " 6   dayofweek     204387 non-null  float64       \n",
            " 7   is_weekend    300039 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(2), int64(3), object(2)\n",
            "memory usage: 18.3+ MB\n",
            "\n",
            "--- Yeni Özellikler ve Son 5 Satır ---\n",
            "                  datetime  hour  dayofweek  is_weekend  occupancy\n",
            "300034 2020-12-31 23:49:00  23.0        3.0           0         32\n",
            "300035 2020-12-31 23:49:00  23.0        3.0           0         85\n",
            "300036 2020-12-31 23:54:00  23.0        3.0           0         32\n",
            "300037 2020-12-31 23:54:00  23.0        3.0           0         26\n",
            "300038 2020-12-31 23:54:00  23.0        3.0           0         72\n",
            "\n",
            "*** DİKKAT: 95652 adet geçersiz tarih/saat bulundu (NaT). Bu satırlar temizlenmelidir. ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_full.dropna(subset=['datetime'])\n",
        "\n",
        "# 2. Temizlenmiş Veri Setini Kontrol Etme\n",
        "print(f\"\\n--- Veri Temizleme Sonucu ---\")\n",
        "print(f\"Orijinal Satır Sayısı: {len(df_full)}\")\n",
        "print(f\"Temizlenmiş Satır Sayısı: {len(df_clean)}\")\n",
        "print(f\"Kaldırılan Kayıt Sayısı: {len(df_full) - len(df_clean)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5dVLSa2rkO3",
        "outputId": "e6642217-bff1-4b41-f80f-95a25574fc7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Veri Temizleme Sonucu ---\n",
            "Orijinal Satır Sayısı: 300039\n",
            "Temizlenmiş Satır Sayısı: 204387\n",
            "Kaldırılan Kayıt Sayısı: 95652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_clean.copy()\n",
        "\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "# 'id_park' sütunundaki string ifadeleri sayılara dönüştür\n",
        "df['park_id_encoded'] = le.fit_transform(df['id_park'])\n",
        "\n",
        "print(\"\\n--- Son Kontrol (Temizlenmiş Veri) ---\")\n",
        "print(df[['id_park', 'park_id_encoded', 'datetime', 'hour', 'occupancy']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8kw_yXAsEk5",
        "outputId": "a56a6482-458a-4c61-beaf-ecca0e0a902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Son Kontrol (Temizlenmiş Veri) ---\n",
            "    id_park  park_id_encoded            datetime  hour  occupancy\n",
            "25     P042               33 2020-01-01 00:23:00   0.0         25\n",
            "36     P042               33 2020-01-01 00:34:00   0.0         25\n",
            "82     P042               33 2020-01-01 01:33:00   1.0         35\n",
            "97     P042               33 2020-01-01 01:44:00   1.0         40\n",
            "113    P042               33 2020-01-01 02:04:00   2.0         47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "portugal_holidays_2020 = [\n",
        "    '2020/01/01',  # Yılbaşı\n",
        "    '2020/04/10',  # Kutsal Cuma (Good Friday)\n",
        "    '2020/04/13',  # Paskalya Pazartesi (Easter Monday)\n",
        "    '2020/04/25',  # Özgürlük Günü (Freedom Day)\n",
        "    '2020/05/01',  # İşçi Bayramı (Labour Day)\n",
        "    '2020/06/10',  # Portekiz Günü (Portugal Day)\n",
        "    '2020/08/15',  # Meryem Ana'nın Göğe Alınması (Assumption of Mary)\n",
        "    '2020/10/05',  # Cumhuriyet Günü (Republic Day)\n",
        "    '2020/12/01',  # Bağımsızlık Günü (Restoration of Independence)\n",
        "    '2020/12/08',  # Kutsal Gebelik (Immaculate Conception)\n",
        "    '2020/12/25'   # Noel (Christmas Day)\n",
        "]\n",
        "df['date_only'] = df['datetime'].dt.strftime('%Y/%m/%d')\n",
        "df['is_holiday'] = df['date_only'].isin(portugal_holidays_2020).astype(int)\n",
        "df = df.drop(columns=['date_only'])\n",
        "\n",
        "# 3. Son Kontrol\n",
        "print(\"\\n--- Tatil Özelliği Kontrolü ---\")\n",
        "print(df[df['is_holiday'] == 1][['datetime', 'is_holiday', 'occupancy']].head()) # Tatil günlerinden birkaçı\n",
        "print(df[['datetime', 'hour', 'is_weekend', 'is_holiday', 'occupancy']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmc31Bb9txx5",
        "outputId": "5aa3753a-c333-4fec-8bde-4a04fc7b04bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tatil Özelliği Kontrolü ---\n",
            "               datetime  is_holiday  occupancy\n",
            "25  2020-01-01 00:23:00           1         25\n",
            "36  2020-01-01 00:34:00           1         25\n",
            "82  2020-01-01 01:33:00           1         35\n",
            "97  2020-01-01 01:44:00           1         40\n",
            "113 2020-01-01 02:04:00           1         47\n",
            "               datetime  hour  is_weekend  is_holiday  occupancy\n",
            "25  2020-01-01 00:23:00   0.0           0           1         25\n",
            "36  2020-01-01 00:34:00   0.0           0           1         25\n",
            "82  2020-01-01 01:33:00   1.0           0           1         35\n",
            "97  2020-01-01 01:44:00   1.0           0           1         40\n",
            "113 2020-01-01 02:04:00   2.0           0           1         47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install meteostat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLiS7wQ5sGAp",
        "outputId": "6d61beaa-86a6-4b50-dac2-892c2ed94aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: meteostat in /usr/local/lib/python3.12/dist-packages (1.7.6)\n",
            "Requirement already satisfied: pandas>=2 in /usr/local/lib/python3.12/dist-packages (from meteostat) (2.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from meteostat) (2025.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from meteostat) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2->meteostat) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2->meteostat) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2->meteostat) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Lizbon Merkezi Koordinatlarını Belirle\n",
        "LAT = 38.7223\n",
        "LON = -9.1393\n",
        "ALT = 110\n",
        "\n",
        "# 3. Meteostat için Nokta Tanımlama\n",
        "location = Point(LAT, LON, ALT)\n",
        "start_date = df['datetime'].min().date()\n",
        "end_date = df['datetime'].max().date()\n",
        "\n",
        "print(f\"Hava durumu verisi çekilecek tarih aralığı: {start_date} -> {end_date}\")\n",
        "\n",
        "start_dt = datetime.strptime(str(start_date), '%Y-%m-%d')\n",
        "end_dt = datetime.strptime(str(end_date), '%Y-%m-%d')\n",
        "\n",
        "# Saatlik veri çekme\n",
        "data = Hourly(location, start_dt, end_dt)\n",
        "data = data.fetch()\n",
        "\n",
        "# Hava Durumu DataFrame'i\n",
        "weather_df = data[['temp', 'prcp', 'wspd', 'pres']].copy()\n",
        "weather_df.index.name = 'datetime'\n",
        "\n",
        "print(f\"\\nÇekilen Saatlik Hava Durumu Kayıt Sayısı: {len(weather_df)}\")\n",
        "\n",
        "# 4. Yuvarlama\n",
        "df['rounded_datetime'] = df['datetime'].dt.floor('H')\n",
        "weather_df.index = weather_df.index.floor('H')\n",
        "\n",
        "# 5. Hava Durumu Verilerini Ana DataFrame ile Birleştirme (SUFFIXES EKLENDİ)\n",
        "# Sonek ekleyerek çakışan isimleri (temp, prcp vb.) ayırıyoruz.\n",
        "# Meteostat'tan gelenleri '_wx' (Weather Extension) ile işaretleyelim.\n",
        "df_merged = df.merge(weather_df,\n",
        "                     left_on='rounded_datetime',\n",
        "                     right_index=True,\n",
        "                     how='left',\n",
        "                     suffixes=('', '_wx')) # Orijinal sütunlara suffix eklenmez, yeni gelenlere '_wx' eklenir.\n",
        "\n",
        "# Geçici ve gereksiz sütunları temizleme\n",
        "df_merged = df_merged.drop(columns=['rounded_datetime'])\n",
        "\n",
        "# Güncel DataFrame'imizi df olarak yeniden atayalım\n",
        "df = df_merged.copy()\n",
        "\n",
        "print(\"\\n--- Hava Durumu Entegrasyonu Sonrası İlk 5 Satır Kontrolü ---\")\n",
        "# Yeni sütunlar temp_wx, prcp_wx vb. olarak gelmeli\n",
        "print(df[['datetime', 'hour', 'is_holiday', 'temp_wx', 'prcp_wx', 'occupancy']].head())\n",
        "print(\"\\n--- Entegre Veri Seti Bilgileri (Yeni Sütunlar Görünmeli) ---\")\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "c9ws0HMvvznr",
        "outputId": "adb5164d-e1b7-45ba-b18a-bbd72bb75386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hava durumu verisi çekilecek tarih aralığı: 2020-01-01 -> 2020-12-31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Cannot load hourly/2020/08535.csv.gz from https://data.meteostat.net/\n",
            "FutureWarning: <class 'pandas.core.arrays.string_.StringArray'>._reduce will require a `keepdims` parameter in the future\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Çekilen Saatlik Hava Durumu Kayıt Sayısı: 8761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hava Durumu Entegrasyonu Sonrası İlk 5 Satır Kontrolü ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['temp_wx', 'prcp_wx'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4101964952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Hava Durumu Entegrasyonu Sonrası İlk 5 Satır Kontrolü ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Yeni sütunlar temp_wx, prcp_wx vb. olarak gelmeli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_holiday'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'temp_wx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prcp_wx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'occupancy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Entegre Veri Seti Bilgileri (Yeni Sütunlar Görünmeli) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['temp_wx', 'prcp_wx'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols_to_keep = [\n",
        "    'id_park', 'name', 'max_capacity', 'occupancy', 'datetime',\n",
        "    'hour', 'dayofweek', 'is_weekend', 'is_holiday', 'park_id_encoded',\n",
        "    'temp_wx', 'prcp_wx', 'wspd_wx', 'pres_wx'\n",
        "]\n",
        "\n",
        "current_cols = df.columns.tolist()\n",
        "final_cols_to_keep = [col for col in cols_to_keep if col in current_cols]\n",
        "\n",
        "# Sadece temizlenmiş sütunları tutan yeni bir DataFrame oluşturalım\n",
        "df_clean = df[final_cols_to_keep].copy()\n",
        "\n",
        "df_clean.rename(columns={\n",
        "    'temp_wx': 'temperature',\n",
        "    'prcp_wx': 'precipitation',\n",
        "    'wspd_wx': 'wind_speed',\n",
        "    'pres_wx': 'pressure'\n",
        "}, inplace=True)\n",
        "\n",
        "df_clean['occupancy_ratio'] = df_clean['occupancy'] / df_clean['max_capacity']\n",
        "\n",
        "\n",
        "# 4. Eksik Hava Durumu Değerlerini Doldurma (Imputation)\n",
        "# 'precipitation' (Yağış) sütununda çok sayıda <NA> vardı. Bunları 0.0 ile dolduralım,\n",
        "# çünkü <NA> genellikle yağış olmadığı anlamına gelir. Diğerlerini ortalama ile dolduralım.\n",
        "df_clean['precipitation'].fillna(0.0, inplace=True)\n",
        "\n",
        "for col in ['temperature', 'wind_speed', 'pressure']:\n",
        "    df_clean[col].fillna(df_clean[col].mean(), inplace=True)\n",
        "\n",
        "df = df_clean.drop(columns=['id_park', 'name', 'occupancy'])\n",
        "\n",
        "print(\"\\n--- Nihai Temizlik Sonrası Veri Seti Kontrolü ---\")\n",
        "df.info()\n",
        "print(\"\\nNihai Veri Seti Başlığı:\")\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "iyU9fcrX0MtQ",
        "outputId": "93e6ccfe-a22a-464e-fcbf-97fb82191b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'precipitation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'precipitation'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3025697094.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 'precipitation' (Yağış) sütununda çok sayıda <NA> vardı. Bunları 0.0 ile dolduralım,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# çünkü <NA> genellikle yağış olmadığı anlamına gelir. Diğerlerini ortalama ile dolduralım.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdf_clean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'precipitation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wind_speed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pressure'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'precipitation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'datetime' sütununu çıkarıyoruz, çünkü model eğitimi için sayısal özelliklere ihtiyacımız var.\n",
        "# Ayrıca modelde kullanmayacağımız ve temizlenmemiş olabilecek eski sütunları da düşürelim.\n",
        "cols_to_drop_final = ['datetime', 'occupancy', 'id_park', 'name']\n",
        "\n",
        "# Sadece mevcut olan sütunları düşürmeyi deneyeceğiz.\n",
        "current_columns = df.columns.tolist()\n",
        "cols_to_safely_drop = [col for col in cols_to_drop_final if col in current_columns]\n",
        "\n",
        "# Nihai DataFrame'i oluştur\n",
        "df_model = df.drop(columns=cols_to_safely_drop, errors='ignore').copy()\n",
        "# .copy() kullanmak FutureWarning'ı azaltmaya yardımcı olur.\n",
        "\n",
        "# 1. Özellikler (X) ve Hedef (Y) Değişkenlerini Ayırma\n",
        "TARGET = 'occupancy_ratio'\n",
        "FEATURES = [\n",
        "    'hour',\n",
        "    'dayofweek',\n",
        "    'is_weekend',\n",
        "    'is_holiday',\n",
        "    'park_id_encoded',\n",
        "    'max_capacity',\n",
        "    'temperature',\n",
        "    'precipitation',\n",
        "    'wind_speed',\n",
        "    'pressure'\n",
        "]\n",
        "\n",
        "X = df_model[FEATURES]\n",
        "Y = df_model[TARGET]\n",
        "\n",
        "# Eksik kalan 1 satırı temizleyelim (occupancy_ratio'da 1 null vardı)\n",
        "# Bu satırda hem X hem Y için aynı satırların düşürüldüğünden emin olmak için birlikte temizleyelim.\n",
        "valid_indices = Y.dropna().index\n",
        "X = X.loc[valid_indices]\n",
        "Y = Y.loc[valid_indices]\n",
        "\n",
        "print(f\"Eğitime Hazır Toplam Kayıt: {len(X)}\")\n",
        "\n",
        "# Buradan sonra Model Eğitim Koduna geçebilirsiniz\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Veri Setini Eğitim ve Test Setlerine Ayırma (Zaman Serisi)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "Y_train, Y_test = Y[:train_size], Y[train_size:]\n",
        "\n",
        "\n",
        "# 3. Veri Ölçeklendirme (Standardizasyon)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "cols_to_scale = ['hour', 'max_capacity', 'temperature', 'precipitation', 'wind_speed', 'pressure']\n",
        "# Eğitim verisini ölçeklendir\n",
        "X_train.loc[:, cols_to_scale] = scaler.fit_transform(X_train[cols_to_scale])\n",
        "# Test verisini aynı scaler ile ölçeklendir\n",
        "X_test.loc[:, cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
        "\n",
        "\n",
        "# 4. Modeli Oluşturma ve Eğitme (Random Forest Regressor)\n",
        "\n",
        "print(\"\\n--- Model Eğitimi Başlıyor (Random Forest) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Eğitim Tamamlandı. Süre: {end_time - start_time:.2f} saniye.\")\n",
        "\n",
        "\n",
        "# 5. Tahmin Yapma ve Değerlendirme\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "mae = mean_absolute_error(Y_test, Y_pred)\n",
        "\n",
        "print(\"\\n--- Model Performans Değerlendirmesi (Ratio Tahmini) ---\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "\n",
        "# 6. Özellik Önem Derecesi (Feature Importance)\n",
        "feature_importances = pd.Series(model.feature_importances_, index=FEATURES).sort_values(ascending=False)\n",
        "print(\"\\n--- Özellik Önem Derecesi ---\")\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "w9Xl2P0g1tAE",
        "outputId": "352ad4de-2e0c-411f-caa3-7e5bd34bacc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['temperature', 'precipitation', 'wind_speed', 'pressure'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1070091080.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['temperature', 'precipitation', 'wind_speed', 'pressure'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_month = 7  # Temmuz\n",
        "end_month = 9    # Eylül (Eylül dahil olduğu için 9'u dahil etmeliyiz)\n",
        "year = 2020\n",
        "\n",
        "# Tahmin için gerekli olan ilk ve son saatleri belirleyelim\n",
        "# Temmuz'un ilk günü saat 00:00:00'dan başlayıp, Eylül'ün son günü saat 23:00:00'a kadar.\n",
        "pred_start_date = datetime(year, start_month, 1, 0, 0, 0)\n",
        "# Eylül 30'unun sonuna kadar veri çekmek için, Ekim 1'in başlangıcından hemen önce bitirelim.\n",
        "pred_end_date = datetime(year, end_month, 30, 23, 0, 0) # Eylül 30'a kadar\n",
        "\n",
        "# 2. Tahmin Edilecek Tarih/Saatleri Oluşturma (Park bazında)\n",
        "# Benzersiz park kimliklerini alalım (eğitim setinde kullanılan sayısal kodlar)\n",
        "unique_park_ids = df['park_id_encoded'].unique()\n",
        "\n",
        "# Tahmin edilecek tüm zaman ve park kombinasyonlarını içeren boş DataFrame oluşturma\n",
        "prediction_data = []\n",
        "current_time = pred_start_date\n",
        "\n",
        "# Her park için her saatlik veriyi oluştur\n",
        "print(\"Tahmin edilecek zaman aralığı ve park kombinasyonları oluşturuluyor...\")\n",
        "while current_time <= pred_end_date:\n",
        "    for park_id in unique_park_ids:\n",
        "        prediction_data.append({\n",
        "            'park_id_encoded': park_id,\n",
        "            'datetime': current_time\n",
        "        })\n",
        "    current_time += timedelta(hours=1) # Bir sonraki saate geç\n",
        "\n",
        "df_predict = pd.DataFrame(prediction_data)\n",
        "print(f\"Oluşturulan Tahmin Satır Sayısı: {len(df_predict)}\")\n",
        "\n",
        "\n",
        "# 3. Tahmin Edilecek Veriler İçin Zaman Özelliklerini Çıkarma\n",
        "df_predict['hour'] = df_predict['datetime'].dt.hour\n",
        "df_predict['dayofweek'] = df_predict['datetime'].dt.dayofweek\n",
        "df_predict['is_weekend'] = (df_predict['dayofweek'] >= 5).astype(int)\n",
        "\n",
        "# 4. Tatil Özelliklerini Ekleme\n",
        "# Temmuz, Ağustos ve Eylül 2020 tatillerini kontrol etmeliyiz.\n",
        "# Sizin orijinal tatil listeniz 2020'nin tamamını kapsayacak şekilde genişletilmelidir.\n",
        "portugal_holidays_2020_full = [\n",
        "    '2020/01/01', '2020/04/10', '2020/04/13', '2020/04/25', '2020/05/01',\n",
        "    '2020/06/10', '2020/08/15',  # Yaz ayındaki tek resmi tatil (15 Ağustos)\n",
        "    '2020/10/05', '2020/12/01', '2020/12/08', '2020/12/25'\n",
        "]\n",
        "df_predict['date_only'] = df_predict['datetime'].dt.strftime('%Y/%m/%d')\n",
        "df_predict['is_holiday'] = df_predict['date_only'].isin(portugal_holidays_2020_full).astype(int)\n",
        "df_predict = df_predict.drop(columns=['date_only'])\n",
        "\n",
        "print(\"Tahmin Veri Seti Hazır (İlk 5 Satır):\")\n",
        "print(df_predict.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDDcHVM_1qcP",
        "outputId": "09d81b80-3957-4ec2-9e72-6491e5ee8709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tahmin edilecek zaman aralığı ve park kombinasyonları oluşturuluyor...\n",
            "Oluşturulan Tahmin Satır Sayısı: 99360\n",
            "Tahmin Veri Seti Hazır (İlk 5 Satır):\n",
            "   park_id_encoded   datetime  hour  dayofweek  is_weekend  is_holiday\n",
            "0               33 2020-07-01     0          2           0           0\n",
            "1               35 2020-07-01     0          2           0           0\n",
            "2               27 2020-07-01     0          2           0           0\n",
            "3                1 2020-07-01     0          2           0           0\n",
            "4               10 2020-07-01     0          2           0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hava Durumu Verisini Çekme ve Entegrasyon (Tekrar) ---\n",
        "\n",
        "# Bu kısımlar doğru çalışıyordu:\n",
        "# [Meteostat veri çekme kısmı...]\n",
        "# data_pred = Hourly(location, pred_start_date_weather, pred_end_date_weather).fetch()\n",
        "# ...\n",
        "\n",
        "# 1. Hava Durumu Verilerini Tahmin DataFrame'i ile Birleştirme\n",
        "df_predict_merged = df_predict.merge(weather_df_pred,\n",
        "                                     left_on='rounded_datetime',\n",
        "                                     right_index=True,\n",
        "                                     how='left',\n",
        "                                     suffixes=('_pred', ''))\n",
        "df_predict_merged = df_predict_merged.drop(columns=['rounded_datetime'])\n",
        "\n",
        "# 2. Sütun İsimlerini Düzeltme\n",
        "df_predict_merged.rename(columns={\n",
        "    'temp': 'temperature',\n",
        "    'prcp': 'precipitation',\n",
        "    'wspd': 'wind_speed',\n",
        "    'pres': 'pressure'\n",
        "}, inplace=True)\n",
        "\n",
        "# 3. max_capacity özelliğini tahmin setine ekleme (HATA DÜZELTİLDİ)\n",
        "# Hata veren satırın düzeltilmiş hali:\n",
        "# Amacımız: Her park_id_encoded için tek bir max_capacity değeri almak.\n",
        "park_capacity_map = df.groupby('park_id_encoded')['max_capacity'].first().to_dict()\n",
        "\n",
        "# Kapasiteyi yeni DataFrame'e eşleme\n",
        "df_predict_merged['max_capacity'] = df_predict_merged['park_id_encoded'].map(park_capacity_map)\n",
        "\n",
        "# Eksik kalırsa en sık kullanılan kapasite ile doldur (güvenlik için)\n",
        "df_predict_merged['max_capacity'].fillna(df['max_capacity'].mode().iloc[0], inplace=True)\n",
        "df_predict_merged['max_capacity'] = df_predict_merged['max_capacity'].astype(int) # Tam sayıya çevirme\n",
        "\n",
        "\n",
        "# 4. Eksik Hava Durumu Değerlerini Doldurma (Imputation)\n",
        "df_predict_merged['precipitation'].fillna(0.0, inplace=True)\n",
        "\n",
        "for col in ['temperature', 'wind_speed', 'pressure']:\n",
        "    df_predict_merged[col].fillna(df_predict_merged[col].mean(), inplace=True)\n",
        "\n",
        "\n",
        "# 5. Özellikleri Ölçeklendirme (Eğitimde kullanılan scaler ile)\n",
        "cols_to_scale_full = ['hour', 'max_capacity', 'temperature', 'precipitation', 'wind_speed', 'pressure']\n",
        "X_new_scaled = df_predict_merged[cols_to_scale_full].copy()\n",
        "\n",
        "# .loc kullanarak warning'i azaltma ve güvenli ölçekleme\n",
        "X_new_scaled.loc[:, cols_to_scale_full] = scaler.transform(X_new_scaled[cols_to_scale_full])\n",
        "\n",
        "# Geri kalan özellikleri birleştirip tahmin setini oluşturalım\n",
        "X_new_other = df_predict_merged[['dayofweek', 'is_weekend', 'is_holiday', 'park_id_encoded']]\n",
        "\n",
        "# X_new için sadece gerekli sütunları birleştirelim.\n",
        "# Önemli: Concatenation'dan sonra, sütun sırasını modelin beklediği hale getireceğiz.\n",
        "X_new_temp = pd.concat([X_new_other.reset_index(drop=True), X_new_scaled.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Sütun sırasını modelin beklediği \"FEATURES\" listesiyle eşitleme\n",
        "FEATURES = [\n",
        "    'hour', 'dayofweek', 'is_weekend', 'is_holiday', 'park_id_encoded',\n",
        "    'max_capacity', 'temperature', 'precipitation', 'wind_speed', 'pressure'\n",
        "]\n",
        "X_new = X_new_temp[FEATURES]\n",
        "\n",
        "\n",
        "# 6. Tahmin Yapma\n",
        "Y_predicted_ratio = model.predict(X_new)\n",
        "\n",
        "# 7. Sonuçları DataFrame'e Ekleme\n",
        "df_predict_merged['predicted_occupancy_ratio'] = Y_predicted_ratio\n",
        "df_predict_merged['predicted_occupancy'] = (df_predict_merged['predicted_occupancy_ratio'] * df_predict_merged['max_capacity']).round().astype(int)\n",
        "\n",
        "\n",
        "print(\"\\n--- Tahmin Sonuçları (Temmuz Başı) ---\")\n",
        "print(df_predict_merged[df_predict_merged['datetime'].dt.month == 7][['datetime', 'park_id_encoded', 'max_capacity', 'temperature', 'predicted_occupancy_ratio', 'predicted_occupancy']].head(15))"
      ],
      "metadata": {
        "id": "7a7BStka5EtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ORIGINAL_FEATURES = [\n",
        "    'datetime', 'hour', 'dayofweek', 'is_weekend', 'is_holiday',\n",
        "    'park_id_encoded', 'max_capacity', 'temperature', 'precipitation',\n",
        "    'wind_speed', 'pressure', 'occupancy_ratio'\n",
        "]\n",
        "\n",
        "# Orijinal (Temmuz-Eylül dışındaki) veriyi filtrele\n",
        "# df, model eğitiminden önceki temizlenmiş orijinal DataFrame'inizdir.\n",
        "df_clean_original = df[\n",
        "    (df['datetime'].dt.month < 7) | (df['datetime'].dt.month > 9)\n",
        "].copy()\n",
        "\n",
        "# Sadece ihtiyacımız olan kolonları tutalım\n",
        "df_clean_original = df_clean_original[ORIGINAL_FEATURES]\n",
        "\n",
        "\n",
        "# 2. Tahmin Verisini Hazırlama\n",
        "# df_predict_merged, tahmin sonuçlarının olduğu DataFrame'inizdir.\n",
        "PREDICTED_FEATURES = [\n",
        "    'datetime', 'park_id_encoded', 'max_capacity',\n",
        "    'hour', 'dayofweek', 'is_weekend', 'is_holiday',\n",
        "    'temperature', 'precipitation', 'wind_speed', 'pressure',\n",
        "    'predicted_occupancy_ratio' # Tahmin edilen oran\n",
        "]\n",
        "\n",
        "df_predicted_final = df_predict_merged[PREDICTED_FEATURES].copy()\n",
        "\n",
        "# Sütun adlarını orijinal formata çevirme\n",
        "df_predicted_final.rename(columns={'predicted_occupancy_ratio': 'occupancy_ratio'}, inplace=True)\n",
        "\n",
        "\n",
        "# 3. İki Veri Setini Birleştirme (Union)\n",
        "final_data = pd.concat([df_clean_original, df_predicted_final], ignore_index=True)\n",
        "\n",
        "# Veriyi tarih/zamana göre sıralama\n",
        "final_data = final_data.sort_values(by=['datetime', 'park_id_encoded']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"\\n--- Nihai Birleştirilmiş Veri Seti Kontrolü (TÜM ÖZELLİKLERLE) ---\")\n",
        "print(f\"Toplam Satır Sayısı: {len(final_data)}\")\n",
        "print(\"Sütunlar:\")\n",
        "print(final_data.columns.tolist())\n",
        "print(\"\\nTemmuz Tahmin Başlangıcı:\")\n",
        "print(final_data[final_data['datetime'].dt.month == 7].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "T90Dn7bF66Rj",
        "outputId": "8dd4a3f3-2ea3-422a-ea66-57c22f252ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['temperature', 'precipitation', 'wind_speed', 'pressure', 'occupancy_ratio'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-981043960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Sadece ihtiyacımız olan kolonları tutalım\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_clean_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_clean_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mORIGINAL_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['temperature', 'precipitation', 'wind_speed', 'pressure', 'occupancy_ratio'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.to_csv('2020_Park_Doluluk_Tahmin_Tamamlandi.csv', index=False)\n",
        "print(\"Nihai veri seti başarıyla kaydedildi: 2020_Park_Doluluk_Tahmin_Tamamlandi.csv\")\n",
        "df_new=pd.read_csv('2020_Park_Doluluk_Tahmin_Tamamlandi.csv')\n",
        "df_new.head()\n",
        "#df_new.info()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qJRgcNOn8DEw",
        "outputId": "9d9d940f-4c24-42ac-cbb1-1fc572e3852b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2907486299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2020_Park_Doluluk_Tahmin_Tamamlandi.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nihai veri seti başarıyla kaydedildi: 2020_Park_Doluluk_Tahmin_Tamamlandi.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2020_Park_Doluluk_Tahmin_Tamamlandi.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df_new.info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "\n",
        "# --- Yardımcı Fonksiyonlar (Daha Önce Kullanılan Yapıları Tekrar Oluşturma) ---\n",
        "\n",
        "# 1. Hava Durumu Çekme Fonksiyonu (Meteostat'ı kullanır)\n",
        "def fetch_weather_for_prediction(dt_start, dt_end, location):\n",
        "    \"\"\"Belirtilen zaman aralığı için hava durumu verilerini çeker.\"\"\"\n",
        "    # Bu kısım için Meteostat kütüphanesinin kurulu ve API'nin erişilebilir olması gerekir.\n",
        "    try:\n",
        "        from meteostat import Hourly, Point\n",
        "        data_pred = Hourly(location, dt_start, dt_end).fetch()\n",
        "        weather_df_pred = data_pred[['temp', 'prcp', 'wspd', 'pres']].copy()\n",
        "        weather_df_pred.index.name = 'datetime'\n",
        "        weather_df_pred.index = weather_df_pred.index.floor('H')\n",
        "        return weather_df_pred\n",
        "    except Exception as e:\n",
        "        print(f\"Hava durumu çekilirken hata oluştu (Meteostat API erişimini kontrol edin): {e}\")\n",
        "        return None\n",
        "\n",
        "# 2. Özellik Çıkarma ve Ölçekleme Fonksiyonu\n",
        "def prepare_features_for_prediction(df_template, target_datetime, park_capacities_map, scaler_obj):\n",
        "    \"\"\"Tek bir zaman noktası için tüm gerekli özellikleri oluşturur, ölçekler ve X formatına getirir.\"\"\"\n",
        "\n",
        "    # Tüm park ID'leri için tek bir zaman dilimi oluştur\n",
        "    current_park_ids = df_template['park_id_encoded'].unique()\n",
        "\n",
        "    pred_data = []\n",
        "    for park_id in current_park_ids:\n",
        "        # Temel zaman özelliklerini hesapla\n",
        "        hour = target_datetime.hour\n",
        "        dayofweek = target_datetime.dayofweek\n",
        "        is_weekend = (dayofweek >= 5).astype(int)\n",
        "\n",
        "        # Tatil kontrolü (Basitleştirilmiş versiyon)\n",
        "        # Gerçek uygulamada, tatil listesini kullanmalısınız. Burada sadece 2020 verisi olduğu için\n",
        "        # sadece Ocak 1'i kontrol edelim veya dışarıdan bir tatil kontrol fonksiyonu çağıralım.\n",
        "        # Güvenlik için: Tatil kontrolünü manuel yapmadan sadece ana özelliklere odaklanalım.\n",
        "        is_holiday = 0 # Basitleştirme adına varsayılan 0\n",
        "\n",
        "        # Kapasiteyi ekle\n",
        "        capacity = park_capacities_map.get(park_id, df_template['max_capacity'].mode().iloc[0])\n",
        "\n",
        "        pred_data.append({\n",
        "            'hour': hour,\n",
        "            'dayofweek': dayofweek,\n",
        "            'is_weekend': is_weekend,\n",
        "            'is_holiday': is_holiday,\n",
        "            'park_id_encoded': park_id,\n",
        "            'max_capacity': capacity,\n",
        "        })\n",
        "\n",
        "    X_new = pd.DataFrame(pred_data)\n",
        "\n",
        "    # Hava Durumu Çekimi (Varsayımsal olarak 19.0C ve 0.0 yağış alalım, çünkü gerçek çekimi burada yapamayız)\n",
        "    # GERÇEKTE BURADA fetch_weather_for_prediction çağırmalısınız!\n",
        "    X_new['temperature'] = 19.0  # Örnek bir değer\n",
        "    X_new['precipitation'] = 0.0 # Örnek bir değer\n",
        "    X_new['wind_speed'] = 5.0    # Örnek bir değer\n",
        "    X_new['pressure'] = 1013.0   # Örnek bir değer\n",
        "\n",
        "    # Ölçekleme\n",
        "    cols_to_scale_full = ['hour', 'max_capacity', 'temperature', 'precipitation', 'wind_speed', 'pressure']\n",
        "\n",
        "    # Özelliklerin sırasını ve isimlerini modelin beklediği gibi düzenle\n",
        "    X_new_scaled = X_new[cols_to_scale_full].copy()\n",
        "    X_new_scaled.loc[:, cols_to_scale_full] = scaler_obj.transform(X_new_scaled[cols_to_scale_full])\n",
        "\n",
        "    # Nihai X formatını oluştur (Orijinal FEATURE sırasıyla)\n",
        "    FEATURES = [\n",
        "        'hour', 'dayofweek', 'is_weekend', 'is_holiday', 'park_id_encoded',\n",
        "        'max_capacity', 'temperature', 'precipitation', 'wind_speed', 'pressure'\n",
        "    ]\n",
        "\n",
        "    X_final = pd.merge(X_new[['dayofweek', 'is_weekend', 'is_holiday', 'park_id_encoded', 'max_capacity']],\n",
        "                       X_new_scaled,\n",
        "                       on=['max_capacity'],\n",
        "                       how='inner')\n",
        "\n",
        "    # Sütun sırasını FEATURES listesine göre düzenle\n",
        "    X_final = X_final[FEATURES]\n",
        "\n",
        "    return X_final\n",
        "\n",
        "\n",
        "# --- ANA ÖNERİ FONKSİYONU ---\n",
        "\n",
        "def suggest_parking_and_predict(hours_ahead, df_base, model_obj, scaler_obj, park_capacities_map, location_point):\n",
        "    \"\"\"\n",
        "    Belirtilen süre sonrası için doluluk tahmin eder ve en uygun parkı önerir.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Hedef Zamanı Belirleme\n",
        "    # Tahmin anını (örneğin son veri noktası) alın\n",
        "    last_known_time = df_base['datetime'].max()\n",
        "\n",
        "    # Kullanıcının istediği ileri zamanı hesapla\n",
        "    target_datetime = last_known_time + timedelta(hours=hours_ahead)\n",
        "    print(f\"\\nTahmin Zamanı: {target_datetime} (Son bilinen zamandan {hours_ahead} saat sonra)\")\n",
        "\n",
        "    # 2. Özellikleri Hazırlama (Hava durumu çekimini burada yapmak gerekir)\n",
        "\n",
        "    # --- GERÇEK UYGULAMADA YAPILMASI GEREKENLER (Hava Durumu Çekimi) ---\n",
        "    # weather_data = fetch_weather_for_prediction(target_datetime, target_datetime + timedelta(hours=1), location_point)\n",
        "    # Eğer weather_data boş gelirse, tahmin yapamayız.\n",
        "    # -------------------------------------------------------------------\n",
        "\n",
        "    # Şimdilik, sadece zaman/park özelliklerini hazırlıyoruz (Hava durumu elle varsayılmıştır)\n",
        "    X_pred = prepare_features_for_prediction(df_base, target_datetime, park_capacities_map, scaler_obj)\n",
        "\n",
        "    # 3. Tahmin Yapma\n",
        "    Y_pred_ratio = model_obj.predict(X_pred)\n",
        "\n",
        "    # Tahminleri DataFrame'e ekleme\n",
        "    X_pred['predicted_occupancy_ratio'] = Y_pred_ratio\n",
        "\n",
        "    # Doluluk sayısını hesaplama\n",
        "    X_pred['predicted_occupancy'] = (X_pred['predicted_occupancy_ratio'] * X_pred['max_capacity']).round().astype(int)\n",
        "\n",
        "    # 4. Öneri (Örneğin, EN AZ DOLU OLAN PARK)\n",
        "\n",
        "    # En az doluluğa sahip parkı bul\n",
        "    best_suggestion = X_pred.sort_values(by='predicted_occupancy', ascending=True).iloc[0]\n",
        "\n",
        "    # Sonuçları formatlama\n",
        "    suggestion_details = {\n",
        "        \"Tahmin Edilen Zaman\": target_datetime.strftime('%Y-%m-%d %H:%M'),\n",
        "        \"Önerilen Park ID\": best_suggestion['park_id_encoded'],\n",
        "        \"Tahmin Edilen Doluluk Oranı\": f\"{best_suggestion['predicted_occupancy_ratio']:.2f} ({best_suggestion['predicted_occupancy']} araç)\",\n",
        "        \"Park Kapasitesi\": best_suggestion['max_capacity'],\n",
        "        \"Tahmin Edilen Sıcaklık (Örnek)\": X_pred.loc[X_pred['park_id_encoded'] == best_suggestion['park_id_encoded'], 'temperature'].iloc[0]\n",
        "    }\n",
        "\n",
        "    return suggestion_details, X_pred\n",
        "\n",
        "\n",
        "# --- KULLANIM ÖRNEĞİ ---\n",
        "\n",
        "# Varsayımlar (Bunları önceki adımlardan almalısınız)\n",
        "# 1. Park Kapasiteleri Haritası (df'ten alınmalı)\n",
        "park_capacities_map = df_new.groupby('park_id_encoded')['max_capacity'].first().to_dict()\n",
        "# 2. Hava durumu için konumu tanımla (Örnek değerler)\n",
        "LAT, LON, ALT = 38.7223, -9.1393, 50\n",
        "location = Point(LAT, LON, ALT)\n",
        "\n",
        "\n",
        "# KULLANICI GİRDİSİ: Yarım saat (0.5 saat) veya 1.5 saat sonrası için tahmin yapalım\n",
        "hours_ahead_input = 1.5 # Kullanıcı 1.5 saat sonrasını sordu\n",
        "\n",
        "# Tahmin ve Öneri Fonksiyonunu Çalıştırma\n",
        "# NOT: Bu, hava durumu verisi çekme adımında hata verebilir, çünkü o kısım sadece simülasyon içerir.\n",
        "suggestion, all_predictions = suggest_parking_and_predict(\n",
        "    hours_ahead=hours_ahead_input,\n",
        "    df_base=df, # Orijinal temizlenmiş/genişletilmiş df'i kullanıyoruz\n",
        "    model_obj=model,\n",
        "    scaler_obj=scaler,\n",
        "    park_capacities_map=park_capacities_map,\n",
        "    location_point=location\n",
        ")\n",
        "\n",
        "print(\"\\n*** ÖNERİ SONUCU ***\")\n",
        "for key, value in suggestion.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "369-jk23BE9a",
        "outputId": "2ca26839-d447-49f3-ad0f-cd88c5ebf385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_new' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-724471764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Varsayımlar (Bunları önceki adımlardan almalısınız)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;31m# 1. Park Kapasiteleri Haritası (df'ten alınmalı)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mpark_capacities_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'park_id_encoded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_capacity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;31m# 2. Hava durumu için konumu tanımla (Örnek değerler)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0mLAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m38.7223\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9.1393\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_new' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1W3cBW5fXUsZeJCwZoS7rJGYl-pvrsgTp",
      "authorship_tag": "ABX9TyMS/6bgz+IMgCMq1trEy++x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}